{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSDS458_Assignment_03_Experiment_B.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCiyk3bZCfMUtatGdB3dQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xixilili/MSDS_458_Public/blob/master/MSDS458_Assignment_03/MSDS458_Assignment_03_Experiment_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments B: RNN: \n",
        "Try several experiments by tweaking (i) architecture (ii) Bidirectional/unidirectional & other hyper parameters, including regularization."
      ],
      "metadata": {
        "id": "OhtHSch636HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Package"
      ],
      "metadata": {
        "id": "LYpO5EAqAJOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SgixklFh2Fb"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from packaging import version\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Process Data"
      ],
      "metadata": {
        "id": "5JmX9O_SAF5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
        "\n",
        "# Example Approaches to Split Data Set\n",
        "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:]','test[:1000]', 'test[1000:]'],\n",
        "dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test[:]'],\n",
        "# dataset, info = tfds.load('ag_news_subset', with_info=True,  split=['train[:114000]','train[114000:]', 'test[:]'],\n",
        "                          as_supervised=True)\n",
        "train_dataset, validation_dataset, test_dataset = dataset\n",
        "# train_dataset, test_dataset = dataset['train'],dataset['test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuL0XQrvh3hy",
        "outputId": "5b1ef263-7463-44b6-873b-4edd261d47f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-02-18 21:05:47.986702: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "I0218 21:05:47.987008 139701657638784 download_and_prepare.py:200] Running download_and_prepare for dataset(s):\n",
            "ag_news_subset\n",
            "I0218 21:05:47.988306 139701657638784 dataset_info.py:361] Load dataset info from /root/tensorflow_datasets/ag_news_subset/1.0.0\n",
            "I0218 21:05:47.989976 139701657638784 download_and_prepare.py:138] download_and_prepare for dataset ag_news_subset/1.0.0...\n",
            "I0218 21:05:47.990235 139701657638784 dataset_builder.py:299] Reusing dataset ag_news_subset (/root/tensorflow_datasets/ag_news_subset/1.0.0)\n",
            "\u001b[1mname: \"ag_news_subset\"\n",
            "description: \"AG is a collection of more than 1 million news articles.\\nNews articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\\nComeToMyHead is an academic news search engine which has been running since July, 2004.\\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\\ninformation retrieval (ranking, search, etc), xml, data compression, data streaming,\\nand any other non-commercial activity.\\nFor more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\\n\\nThe AG\\'s news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\\nIt is used as a text classification benchmark in the following paper:\\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\\n\\nThe AG\\'s news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\\nEach class contains 30,000 training samples and 1,900 testing samples.\\nThe total number of training samples is 120,000 and testing 7,600.\"\n",
            "citation: \"@misc{zhang2015characterlevel,\\n    title={Character-level Convolutional Networks for Text Classification},\\n    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\\n    year={2015},\\n    eprint={1509.01626},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.LG}\\n}\"\n",
            "location {\n",
            "  urls: \"https://arxiv.org/abs/1509.01626\"\n",
            "}\n",
            "splits {\n",
            "  name: \"test\"\n",
            "  shard_lengths: 7600\n",
            "  num_bytes: 2226751\n",
            "}\n",
            "splits {\n",
            "  name: \"train\"\n",
            "  shard_lengths: 120000\n",
            "  num_bytes: 35301386\n",
            "}\n",
            "supervised_keys {\n",
            "  input: \"description\"\n",
            "  output: \"label\"\n",
            "}\n",
            "version: \"1.0.0\"\n",
            "download_size: 11784327\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "3TvvsyRLn8mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Vocab"
      ],
      "metadata": {
        "id": "1lSxL5sP_8gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_vocab(train_dataset,encoder):\n",
        "  doc_sizes = []\n",
        "  corpus = []\n",
        "  for example, _ in train_dataset.as_numpy_iterator():\n",
        "    enc_example = encoder(example)\n",
        "    doc_sizes.append(len(enc_example))\n",
        "    corpus+=list(enc_example.numpy())\n",
        "  return  corpus, doc_sizes"
      ],
      "metadata": {
        "id": "ThWn4Vuj_9Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile, Train Model"
      ],
      "metadata": {
        "id": "90gEMg-5AT8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def namestr(obj, namespace):\n",
        "  for name in namespace:\n",
        "    if namespace[name] is obj:\n",
        "      return name"
      ],
      "metadata": {
        "id": "ovWMn7oP49ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "\n",
        "def compile_train_model(model, epoch):\n",
        "  #compile model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(1e-4)\n",
        "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() # if we set from_logits=True we don not have specify a softmax activation function in the last layer\n",
        "              ,metrics=['accuracy'])\n",
        "\n",
        "  start_datetime = dt.datetime.now()\n",
        "\n",
        "  #train model  \n",
        "  history = model.fit(train_dataset\n",
        "                    ,epochs = epoch\n",
        "                    ,validation_data=validation_dataset\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]\n",
        "                    )\n",
        "\n",
        "\n",
        "  #evaluate model\n",
        "  loss, accuracy = model.evaluate(test_dataset)\n",
        "  print('test set accuracy: ', accuracy * 100)\n",
        "\n",
        "  runtime = (dt.datetime.now() - start_datetime).total_seconds()\n",
        "\n",
        "  #training and validation performance metrix\n",
        "  history_dict = history.history\n",
        "  history_df=pd.DataFrame(history_dict)\n",
        "\n",
        "  #loss and accuracy for training and validation data\n",
        "  losses = history.history['loss']\n",
        "  accs = history.history['accuracy']\n",
        "  val_losses = history.history['val_loss']\n",
        "  val_accs = history.history['val_accuracy']\n",
        "  epochs = len(losses)\n",
        "\n",
        "  result = history_df.tail(1)\n",
        "  result['test_loss'] = loss\n",
        "  result['test_accuracy'] = accuracy\n",
        "  result['process_time'] = runtime\n",
        "  result['epochs_setting'] = epoch  \n",
        "  result['epochs_actual'] = epochs    \n",
        "\n",
        "  plt.figure(figsize=(16, 4))\n",
        "  for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "      plt.subplot(1, 2, i + 1)\n",
        "      plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "      plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "      plt.legend()\n",
        "      plt.title('{0} with {1} epochs'.format(namestr(model, globals()), epoch))   \n",
        "  plt.show()  \n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "UjhVCFgv49_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN"
      ],
      "metadata": {
        "id": "_GCY9AFnAOZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "rPRzfy0WAiHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "vocab = np.array(encoder.get_vocabulary());"
      ],
      "metadata": {
        "id": "0VTr_D3Ih58z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, doc_sizes = explore_vocab(train_dataset,encoder)"
      ],
      "metadata": {
        "id": "CLxwhIkt_xnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "num_vocab_words_in_corpus =len(vocab)\n",
        "\n",
        "num_words =len(corpus)\n",
        "num_articles =len(doc_sizes)\n",
        "min_token_in_a_article = min(doc_sizes)\n",
        "max_token_in_a_article = max(doc_sizes)  \n",
        "\n",
        "print(num_vocab_words_in_corpus)\n",
        "print(num_words)\n",
        "print(num_articles)\n",
        "print(min_token_in_a_article)\n",
        "print(max_token_in_a_article)"
      ],
      "metadata": {
        "id": "sgfSVziC45oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model"
      ],
      "metadata": {
        "id": "VfFgnFGaAZMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleRNN = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True)\n",
        "                              ,tf.keras.layers.RNN(units = 64)\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(4,activation='softmax')   # num_classes = 4\n",
        "])\n"
      ],
      "metadata": {
        "id": "QQjvzY9i8tvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Multiplelayer_simpleRNN = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True)\n",
        "                              ,tf.keras.layers.RNN(units = 64, return_sequences=True)                              \n",
        "                              ,tf.keras.layers.RNN(units = 64)\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(4,activation='softmax')   # num_classes = 4\n",
        "])"
      ],
      "metadata": {
        "id": "bCrG8NNdDLd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BidirectionalRNN = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.RNN(64)) \n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(4,activation='softmax')   # num_classes = 4\n",
        "])"
      ],
      "metadata": {
        "id": "bXIenFUhC27z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Multiplelayer_BidirectionalRNN = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.RNN(64,  return_sequences=True))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.RNN(32))\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dense(4,activation='softmax')   # num_classes = 4\n",
        "])"
      ],
      "metadata": {
        "id": "G-jH8Ui_D24M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Multiplelayer_BidirectionalRNN_Dropout = tf.keras.Sequential([\n",
        "                              encoder\n",
        "                              ,tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),output_dim=64,mask_zero=True)\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.RNN(64, return_sequences=True, recurrent_dropout=0.25))\n",
        "                              ,tf.keras.layers.Bidirectional(tf.keras.layers.RNN(32))\n",
        "                              ,tf.keras.layers.Dense(64, activation='relu')\n",
        "                              ,tf.keras.layers.Dropout(0.5)                              \n",
        "                              ,tf.keras.layers.Dense(4,activation='softmax')   # num_classes = 4\n"
      ],
      "metadata": {
        "id": "wLTuacyBENMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent dropout—This is a variant of dropout, used to fight overfitting in recurrent layers.\n",
        "Stacking recurrent layers—This increases the representational power of the model (at the cost of higher computational loads).\n",
        "Bidirectional recurrent layers—These present the same information to a recurrent network in different ways, increasing accuracy and mitigating forgetting issues."
      ],
      "metadata": {
        "id": "Gm4ERXjtE8gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple RNN"
      ],
      "metadata": {
        "id": "WiBq5Zt9DXPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_simpleRNN = compile_train_model(simpleRNN, 1)"
      ],
      "metadata": {
        "id": "sns01SJr4vA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_simpleRNN['vocab_size'] = VOCAB_SIZE \n",
        "result_simpleRNN['num_words'] = num_words \n",
        "result_simpleRNN['num_articles'] = num_articles\n",
        "result_simpleRNN['min_token_in_a_article'] = min_token_in_a_article \n",
        "result_simpleRNN['max_token_in_a_article'] = max_token_in_a_article \n",
        "new_col = ['SimpleRNN']   \n",
        "result_simpleRNN.insert(loc=0, column='Model', value=new_col)"
      ],
      "metadata": {
        "id": "zEyeashQ5Rtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev_results_df = pd.read_pickle('resultsAa.pkl')\n",
        "assignment_result_table = prev_results_df.append(result_simpleRNN,ignore_index=True)\n",
        "assignment_result_table"
      ],
      "metadata": {
        "id": "edYIaB3C_Khx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplelayer RNN"
      ],
      "metadata": {
        "id": "3nKgSA8KctEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_msimpleRNN = compile_train_model(Multiplelayer_simpleRNN, 1)"
      ],
      "metadata": {
        "id": "f8VQaa2MctEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_msimpleRNN['vocab_size'] = VOCAB_SIZE \n",
        "result_msimpleRNN['num_words'] = num_words \n",
        "result_msimpleRNN['num_articles'] = num_articles\n",
        "result_msimpleRNN['min_token_in_a_article'] = min_token_in_a_article \n",
        "result_msimpleRNN['max_token_in_a_article'] = max_token_in_a_article \n",
        "new_col = ['Multiplelayer RNN']   \n",
        "result_msimpleRNN.insert(loc=0, column='Model', value=new_col)"
      ],
      "metadata": {
        "id": "MvuXp3qPctEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assignment_result_table = assignment_result_table.append(result_msimpleRNN,ignore_index=True)\n",
        "assignment_result_table"
      ],
      "metadata": {
        "id": "HgAchME8ctEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional RNN"
      ],
      "metadata": {
        "id": "Ug6DZtNtcxfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_BiRNN = compile_train_model(BidirectionalRNN, 1)"
      ],
      "metadata": {
        "id": "3HyHNkoocxfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_BiRNN['vocab_size'] = VOCAB_SIZE \n",
        "result_BiRNN['num_words'] = num_words \n",
        "result_BiRNN['num_articles'] = num_articles\n",
        "result_BiRNN['min_token_in_a_article'] = min_token_in_a_article \n",
        "result_BiRNN['max_token_in_a_article'] = max_token_in_a_article \n",
        "new_col = ['Bidirectional RNN']   \n",
        "result_BiRNN.insert(loc=0, column='Model', value=new_col)"
      ],
      "metadata": {
        "id": "ufjOCOP9cxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assignment_result_table = assignment_result_table.append(result_BiRNN,ignore_index=True)\n",
        "assignment_result_table"
      ],
      "metadata": {
        "id": "_gNgLxjGcxfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplelayer Bidirectional RNN"
      ],
      "metadata": {
        "id": "jfMy7L0Tcy4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_MBiRNN = compile_train_model(Multiplelayer_BidirectionalRNN, 1)"
      ],
      "metadata": {
        "id": "qrRkMdgjcy4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_MBiRNN['vocab_size'] = VOCAB_SIZE \n",
        "result_MBiRNN['num_words'] = num_words \n",
        "result_MBiRNN['num_articles'] = num_articles\n",
        "result_MBiRNN['min_token_in_a_article'] = min_token_in_a_article \n",
        "result_MBiRNN['max_token_in_a_article'] = max_token_in_a_article \n",
        "new_col = ['Multiplelayer Bidirectional RNN']   \n",
        "result_MBiRNN.insert(loc=0, column='Model', value=new_col)"
      ],
      "metadata": {
        "id": "CPPYIja5cy4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assignment_result_table = assignment_result_table.append(result_MBiRNN,ignore_index=True)\n",
        "assignment_result_table"
      ],
      "metadata": {
        "id": "c3vqRHPNcy4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplelayer Bidirectional RNN Dropout"
      ],
      "metadata": {
        "id": "KDN8wn-nczln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_MBiDRNN = compile_train_model(Multiplelayer_BidirectionalRNN_Dropout, 1)"
      ],
      "metadata": {
        "id": "rMLLVXqvczlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_MBiDRNN['vocab_size'] = VOCAB_SIZE \n",
        "result_MBiDRNN['num_words'] = num_words \n",
        "result_MBiDRNN['num_articles'] = num_articles\n",
        "result_MBiDRNN['min_token_in_a_article'] = min_token_in_a_article \n",
        "result_MBiDRNN['max_token_in_a_article'] = max_token_in_a_article \n",
        "new_col = ['Multiplelayer Bidirectional RNN with Dropout']   \n",
        "result_MBiDRNN.insert(loc=0, column='Model', value=new_col)"
      ],
      "metadata": {
        "id": "i-0xy7J8czlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assignment_result_table = assignment_result_table.append(result_MBiDRNN,ignore_index=True)\n",
        "assignment_result_table"
      ],
      "metadata": {
        "id": "kBLqrBX-czlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}